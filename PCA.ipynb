{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reference: PCA course by Imperial College London on Coursera\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    \"\"\"Normalize the given dataset X\n",
    "    Args:\n",
    "        X: ndarray, dataset, each row corresponds to measurements for a sample/subject\n",
    "    \n",
    "    Returns:\n",
    "        (Xbar, mean, std): tuple of ndarray, Xbar is the normalized dataset\n",
    "        with mean 0 and standard deviation 1; mean and std are the \n",
    "        mean and standard deviation respectively.\n",
    "    \"\"\"\n",
    "    mu = np.mean(X, axis=0) \n",
    "    std = np.std(X, axis=0)\n",
    "    std_filled = std.copy()\n",
    "    std_filled[std==0] = 1. # handle the situation where sd = 0\n",
    "    Xbar = (X - mu) / std_filled               \n",
    "    return Xbar, mu, std\n",
    "\n",
    "def eig(S):\n",
    "    \"\"\"Compute the eigenvalues and corresponding eigenvectors \n",
    "        for the covariance matrix S.\n",
    "    Args:\n",
    "        S: ndarray, covariance matrix\n",
    "    \n",
    "    Returns:\n",
    "        (eigvals, eigvecs): ndarray, the eigenvalues and eigenvectors\n",
    "    \n",
    "    Note:\n",
    "        the eigenvals and eigenvecs should be sorted in descending\n",
    "        order of the eigen values\n",
    "    \"\"\"\n",
    "    eig_vals, eig_vecs = np.linalg.eig(S)\n",
    "    order = np.argsort(-eig_vals)\n",
    "    return eig_vals[order], eig_vecs[:,order]\n",
    "\n",
    "def projection_matrix(B):\n",
    "    \"\"\"Compute the projection matrix onto the space spanned by `B`\n",
    "    Args:\n",
    "        B: ndarray of dimension (D, M), the basis for the subspace\n",
    "    \n",
    "    Returns:\n",
    "        The projection matrix\n",
    "    \"\"\"\n",
    "    return B @ B.T # since (B.T @ B).I = eye, we are essentially calculating B @ (B.T @ B).I @ B.T\n",
    "\n",
    "def PCA(X, num_components):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: ndarray of size (N, D), where D is the dimension of the data,\n",
    "           and N is the number of datapoints\n",
    "        num_components: the number of principal components to use.\n",
    "    Returns:\n",
    "        X_reconstruct: ndarray of the reconstruction\n",
    "        of X from the first `num_components` principal components.\n",
    "    \"\"\"\n",
    "    # get covariance matrix, X.T because we are more comfortable dealing with \n",
    "    # data matrix where each column corresponds to a subject\n",
    "    S = np.cov(X.T) \n",
    "\n",
    "    eig_vals, eig_vecs = eig(S) # find eigenvevtors\n",
    "    \n",
    "    proj = projection_matrix(eig_vecs[:,:num_components]) # projection matrix\n",
    "    \"\"\"\n",
    "    In Lay, Lay and McDonald's notation, let's call X.T simply X\n",
    "            We have Y = P.T @ X, and since \n",
    "                    X = P   @ Y,\n",
    "    To get back X,  X = P @ P.T @ X\n",
    "    \"\"\"\n",
    "    # Note our code is NOT in Lay, Lay and McDonald's notation, so eig_vecs := P, X.T := X\n",
    "    # And, this time we get back X_hat since eig_vecs has been rank-reduced to B\n",
    "    # Another way to think about this:\n",
    "    # For each data point x_i in X.T, project x_i onto the new eigenbasis.\n",
    "    return (proj@X.T).T "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
